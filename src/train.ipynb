{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import numpy as np\n",
    "from gymnasium.spaces import Box, Space\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from tabulate import tabulate\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers.legacy import Adam\n",
    "\n",
    "from poke_env.environment.abstract_battle import AbstractBattle\n",
    "from poke_env.environment import Move\n",
    "from poke_env.player import (\n",
    "    Player,\n",
    "    Gen8EnvSinglePlayer,\n",
    "    MaxBasePowerPlayer,\n",
    "    ObsType,\n",
    "    RandomPlayer,\n",
    "    SimpleHeuristicsPlayer,\n",
    "    background_cross_evaluate,\n",
    "    background_evaluate_player,\n",
    "    wrap_for_old_gym_api,\n",
    ")\n",
    "from poke_env.data import GenData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/richardzhan/cs/15888/poke\n",
      "\u001b[1m\u001b[36mdata\u001b[m\u001b[m                \u001b[1m\u001b[36mpokemon-showdown\u001b[m\u001b[m    \u001b[1m\u001b[36mscripts\u001b[m\u001b[m\n",
      "poke.code-workspace \u001b[1m\u001b[36mpython\u001b[m\u001b[m              \u001b[1m\u001b[36msrc\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "%cd ~/cs/15888/poke\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from rzlib.env import embed\n",
    "from rzlib.env.simple_rl_player import SimpleRLPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_battle_format = \"gen8ou\"\n",
    "_team1_fname = \"./data/team1.txt\"\n",
    "_team2_fname = \"./data/team2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_team(fname):\n",
    "    # load_bot()\n",
    "    with open(fname, \"r\") as f:\n",
    "        return \"\".join(f.readlines())\n",
    "\n",
    "def create_random_player():\n",
    "    return RandomPlayer(battle_format=_battle_format, team=load_team(_team1_fname))\n",
    "\n",
    "def create_max_power_player():\n",
    "    return MaxBasePowerPlayer(battle_format=_battle_format, team=load_team(_team1_fname))\n",
    "\n",
    "def create_rl_player(opponent, **kwargs):\n",
    "    return SimpleRLPlayer(\n",
    "        battle_format=_battle_format,\n",
    "        opponent=opponent,\n",
    "        start_challenging=True,\n",
    "        team=load_team(_team2_fname),\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "def create_rl_env_random(sanity_check=True):\n",
    "    print(\"create_rl_env_random()...\")\n",
    "    if sanity_check:\n",
    "        # First test the environment to ensure the class is consistent\n",
    "        # with the OpenAI API\n",
    "        sanity_check_env = create_rl_player(opponent=create_random_player())\n",
    "        check_env(sanity_check_env)\n",
    "        sanity_check_env.close()\n",
    "\n",
    "    # Create one environment for training and one for evaluation\n",
    "    train_env = create_rl_player(opponent=create_random_player())\n",
    "    train_env = wrap_for_old_gym_api(train_env)\n",
    "\n",
    "    eval_env = create_rl_player(opponent=create_random_player())\n",
    "    eval_env = wrap_for_old_gym_api(eval_env)\n",
    "\n",
    "    return train_env, eval_env\n",
    "\n",
    "\n",
    "def create_model(train_env, memory_limit=10000, anneal_steps=10000):\n",
    "    print(\"create_model()...\")\n",
    "    # Compute dimensions\n",
    "    n_action = train_env.action_space.n\n",
    "    input_shape = (1,) + train_env.observation_space.shape\n",
    "\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation=\"elu\", input_shape=input_shape))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=\"elu\"))\n",
    "    model.add(Dense(128, activation=\"elu\"))\n",
    "    model.add(Dense(n_action, activation=\"linear\"))\n",
    "\n",
    "    # Defining the DQN\n",
    "    memory = SequentialMemory(limit=memory_limit, window_length=1)\n",
    "\n",
    "    policy = LinearAnnealedPolicy(\n",
    "        EpsGreedyQPolicy(),\n",
    "        attr=\"eps\",\n",
    "        value_max=1.0,\n",
    "        value_min=0.01,\n",
    "        value_test=0.0,\n",
    "        nb_steps=anneal_steps,\n",
    "    )\n",
    "\n",
    "    dqn = DQNAgent(\n",
    "        model=model,\n",
    "        nb_actions=n_action,\n",
    "        policy=policy,\n",
    "        memory=memory,\n",
    "        nb_steps_warmup=1000,\n",
    "        gamma=0.99,\n",
    "        target_model_update=100,\n",
    "        delta_clip=0.01,\n",
    "        enable_double_dqn=True,\n",
    "    )\n",
    "    dqn.compile(Adam(learning_rate=0.001), metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "    return dqn\n",
    "\n",
    "def train_model(*, dqn: DQNAgent, train_env, num_steps=10000):\n",
    "    print(\"train_model()...\", num_steps)\n",
    "    dqn.fit(train_env, nb_steps=num_steps)\n",
    "    train_env.close()\n",
    "\n",
    "def eval_model(*, agent: DQNAgent, eval_env, opponent: Player, num_eval_episodes=100):\n",
    "    print(\"eval_model()...\")\n",
    "\n",
    "    eval_env.reset_env(restart=True, opponent=opponent)\n",
    "    print(f\"Results against {opponent.username}:\")\n",
    "    agent.test(eval_env, nb_episodes=num_eval_episodes, verbose=False, visualize=False)\n",
    "    print(\n",
    "        f\"DQN Evaluation: {eval_env.n_won_battles} victories out of {eval_env.n_finished_battles} episodes\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_rl_env_random()...\n",
      "create_model()...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 10:28:13.974186: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_26_1/kernel/Assign' id:5197 op device:{requested: '', assigned: ''} def:{{{node dense_26_1/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_26_1/kernel, dense_26_1/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_rl_env_random()...\n",
      "train_model()... 1000000\n",
      "Training for 1000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    1/10000 [..............................] - ETA: 33:17 - reward: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 10:28:16.512349: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_27/BiasAdd' id:5113 op device:{requested: '', assigned: ''} def:{{{node dense_27/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_27/MatMul, dense_27/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-11-30 10:28:16.597761: W tensorflow/c/c_api.cc:305] Operation '{name:'total_24/Assign' id:5332 op device:{requested: '', assigned: ''} def:{{{node total_24/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](total_24, total_24/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000/10000 [==>...........................] - ETA: 3:29 - reward: -4.1000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 10:28:39.992987: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_27_1/BiasAdd' id:5231 op device:{requested: '', assigned: ''} def:{{{node dense_27_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_27_1/MatMul, dense_27_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-11-30 10:28:40.116312: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_27/AddN' id:5463 op device:{requested: '', assigned: ''} def:{{{node loss_27/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_27/mul, loss_27/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-11-30 10:28:40.190979: W tensorflow/c/c_api.cc:305] Operation '{name:'training_12/Adam/dense_24/kernel/v/Assign' id:5694 op device:{requested: '', assigned: ''} def:{{{node training_12/Adam/dense_24/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_12/Adam/dense_24/kernel/v, training_12/Adam/dense_24/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 289s 29ms/step - reward: -2.4000\n",
      "882 episodes - episode_reward: -27.211 [-100.000, 100.000] - loss: 0.111 - mae: 4.632 - mean_q: 5.801 - mean_eps: 0.456\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 287s 29ms/step - reward: -0.6800\n",
      "1012 episodes - episode_reward: -6.719 [-100.000, 100.000] - loss: 0.110 - mae: 5.481 - mean_q: 5.680 - mean_eps: 0.010\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 290s 29ms/step - reward: 1.9800\n",
      "1308 episodes - episode_reward: 15.138 [-100.000, 100.000] - loss: 0.139 - mae: 21.644 - mean_q: 22.797 - mean_eps: 0.010\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 291s 29ms/step - reward: 2.5200\n",
      "1310 episodes - episode_reward: 19.237 [-100.000, 100.000] - loss: 0.152 - mae: 24.242 - mean_q: 24.682 - mean_eps: 0.010\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 287s 29ms/step - reward: 4.7100\n",
      "1313 episodes - episode_reward: 35.872 [-100.000, 100.000] - loss: 0.160 - mae: 29.797 - mean_q: 30.816 - mean_eps: 0.010\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 295s 29ms/step - reward: 5.2410\n",
      "1338 episodes - episode_reward: 39.163 [-100.000, 100.000] - loss: 0.167 - mae: 35.393 - mean_q: 34.798 - mean_eps: 0.010\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 300s 30ms/step - reward: 5.5280\n",
      "1345 episodes - episode_reward: 41.115 [-100.000, 100.000] - loss: 0.172 - mae: 38.417 - mean_q: 36.408 - mean_eps: 0.010\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 298s 30ms/step - reward: 5.8710\n",
      "1305 episodes - episode_reward: 44.981 [-100.000, 100.000] - loss: 0.175 - mae: 41.185 - mean_q: 39.125 - mean_eps: 0.010\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 298s 30ms/step - reward: 6.2500\n",
      "1319 episodes - episode_reward: 47.384 [-100.000, 100.000] - loss: 0.170 - mae: 46.763 - mean_q: 45.112 - mean_eps: 0.010\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 300s 30ms/step - reward: 6.8500\n",
      "1305 episodes - episode_reward: 52.490 [-100.000, 100.000] - loss: 0.162 - mae: 48.585 - mean_q: 45.894 - mean_eps: 0.010\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 299s 30ms/step - reward: 6.2910\n",
      "1183 episodes - episode_reward: 53.170 [-100.000, 100.000] - loss: 0.158 - mae: 57.774 - mean_q: 55.357 - mean_eps: 0.010\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 295s 30ms/step - reward: 6.5990\n",
      "1220 episodes - episode_reward: 54.098 [-100.000, 100.000] - loss: 0.149 - mae: 58.524 - mean_q: 56.758 - mean_eps: 0.010\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 293s 29ms/step - reward: 8.1910\n",
      "1251 episodes - episode_reward: 65.468 [-100.000, 100.000] - loss: 0.145 - mae: 61.262 - mean_q: 59.990 - mean_eps: 0.010\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 299s 30ms/step - reward: 7.8590\n",
      "1284 episodes - episode_reward: 61.215 [-100.000, 100.000] - loss: 0.137 - mae: 62.723 - mean_q: 61.766 - mean_eps: 0.010\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 299s 30ms/step - reward: 7.8790\n",
      "1286 episodes - episode_reward: 61.275 [-100.000, 100.000] - loss: 0.132 - mae: 64.242 - mean_q: 63.946 - mean_eps: 0.010\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      " 4935/10000 [=============>................] - ETA: 2:39 - reward: 8.0061done, took 4576.640 seconds\n",
      "create_rl_env_random()...\n",
      "eval_model()...\n",
      "Results against RandomPlayer 65:\n",
      "DQN Evaluation: 75 victories out of 100 episodes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_env, eval_env = create_rl_env_random()\n",
    "\n",
    "# create the model\n",
    "dqn = create_model(\n",
    "    train_env=train_env,\n",
    "    anneal_steps=10000,\n",
    "    memory_limit=100000,\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "# for epoch in range(10):\n",
    "#     print(f\"epoch {epoch}...\")\n",
    "\n",
    "train_env, _ = create_rl_env_random()\n",
    "train_model(\n",
    "    dqn=dqn, \n",
    "    train_env=train_env,\n",
    "    num_steps=1000000\n",
    ")\n",
    "\n",
    "# Evaluating the model\n",
    "## recreate eval_env (sometimes breaks if the training takes too long)\n",
    "_, eval_env = create_rl_env_random()\n",
    "eval_model(\n",
    "    agent=dqn,\n",
    "    eval_env=eval_env,\n",
    "    opponent=create_random_player(),\n",
    "    num_eval_episodes=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_rl_env_random()...\n",
      "Results against RandomPlayer 70:\n",
      "DQN Evaluation: 78 victories out of 100 episodes\n"
     ]
    }
   ],
   "source": [
    "opponent = create_random_player()\n",
    "\n",
    "train_env, eval_env = create_rl_env_random()\n",
    "eval_env.reset_env(restart=True, opponent=opponent)\n",
    "print(f\"Results against {opponent.username}:\")\n",
    "dqn.test(eval_env, nb_episodes=100, verbose=False, visualize=False)\n",
    "print(\n",
    "    f\"DQN Evaluation: {eval_env.n_won_battles} victories out of {eval_env.n_finished_battles} episodes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/richardzhan/cs/15888/poke/src/train.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/richardzhan/cs/15888/poke/src/train.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m eval_env\u001b[39m.\u001b[39;49mclose()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/poke/lib/python3.10/site-packages/poke_env/player/openai_api.py:439\u001b[0m, in \u001b[0;36mOpenAIGymEnv.close\u001b[0;34m(self, purge)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_battle \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mcurrent_battle\n\u001b[1;32m    436\u001b[0m closing_task \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mrun_coroutine_threadsafe(\n\u001b[1;32m    437\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop_challenge_loop(purge\u001b[39m=\u001b[39mpurge), POKE_LOOP\n\u001b[1;32m    438\u001b[0m )\n\u001b[0;32m--> 439\u001b[0m closing_task\u001b[39m.\u001b[39;49mresult()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/poke/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/poke/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poke",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
