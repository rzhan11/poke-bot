{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import numpy as np\n",
    "from gym.spaces import Box, Space\n",
    "from gym.utils.env_checker import check_env\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from tabulate import tabulate\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers.legacy import Adam\n",
    "\n",
    "from poke_env.environment.abstract_battle import AbstractBattle\n",
    "from poke_env.environment import Move\n",
    "from poke_env.player import (\n",
    "    Player,\n",
    "    Gen8EnvSinglePlayer,\n",
    "    MaxBasePowerPlayer,\n",
    "    ObsType,\n",
    "    RandomPlayer,\n",
    "    SimpleHeuristicsPlayer,\n",
    "    background_cross_evaluate,\n",
    "    background_evaluate_player,\n",
    "    wrap_for_old_gym_api,\n",
    ")\n",
    "from poke_env.data import GenData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/richardzhan/cs/15888/poke\n",
      "\u001b[1m\u001b[36mdata\u001b[m\u001b[m                \u001b[1m\u001b[36mpokemon-showdown\u001b[m\u001b[m    \u001b[1m\u001b[36mscripts\u001b[m\u001b[m\n",
      "poke.code-workspace \u001b[1m\u001b[36mpython\u001b[m\u001b[m              \u001b[1m\u001b[36msrc\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "%cd ~/cs/15888/poke\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from rzlib.env import embed\n",
    "from rzlib.env.simple_rl_player import SimpleRLPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_battle_format = \"gen8ou\"\n",
    "_team1_fname = \"./data/team1.txt\"\n",
    "_team2_fname = \"./data/team2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_team(fname):\n",
    "    # load_bot()\n",
    "    with open(fname, \"r\") as f:\n",
    "        return \"\".join(f.readlines())\n",
    "\n",
    "def create_random_player():\n",
    "    return RandomPlayer(battle_format=_battle_format, team=load_team(_team1_fname))\n",
    "\n",
    "def create_max_power_player():\n",
    "    return MaxBasePowerPlayer(battle_format=_battle_format, team=load_team(_team1_fname))\n",
    "\n",
    "def create_rl_player(opponent, **kwargs):\n",
    "    return SimpleRLPlayer(\n",
    "        battle_format=_battle_format,\n",
    "        opponent=opponent,\n",
    "        start_challenging=True,\n",
    "        team=load_team(_team2_fname),\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "def create_rl_env_random(sanity_check=True):\n",
    "    print(\"create_rl_env_random()...\")\n",
    "    if sanity_check:\n",
    "        # First test the environment to ensure the class is consistent\n",
    "        # with the OpenAI API\n",
    "        sanity_check_env = create_rl_player(opponent=create_random_player())\n",
    "        check_env(sanity_check_env)\n",
    "        sanity_check_env.close()\n",
    "\n",
    "    # Create one environment for training and one for evaluation\n",
    "    train_env = create_rl_player(opponent=create_random_player())\n",
    "    train_env = wrap_for_old_gym_api(train_env)\n",
    "\n",
    "    eval_env = create_rl_player(opponent=create_random_player())\n",
    "    eval_env = wrap_for_old_gym_api(eval_env)\n",
    "\n",
    "    return train_env, eval_env\n",
    "\n",
    "\n",
    "def create_model(train_env, memory_limit=10000, anneal_steps=10000):\n",
    "    print(\"create_model()...\")\n",
    "    # Compute dimensions\n",
    "    n_action = train_env.action_space.n\n",
    "    input_shape = (1,) + train_env.observation_space.shape\n",
    "\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation=\"elu\", input_shape=input_shape))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=\"elu\"))\n",
    "    model.add(Dense(128, activation=\"elu\"))\n",
    "    model.add(Dense(n_action, activation=\"linear\"))\n",
    "\n",
    "    # Defining the DQN\n",
    "    memory = SequentialMemory(limit=memory_limit, window_length=1)\n",
    "\n",
    "    policy = LinearAnnealedPolicy(\n",
    "        EpsGreedyQPolicy(),\n",
    "        attr=\"eps\",\n",
    "        value_max=1.0,\n",
    "        value_min=0.01,\n",
    "        value_test=0.0,\n",
    "        nb_steps=anneal_steps,\n",
    "    )\n",
    "\n",
    "    dqn = DQNAgent(\n",
    "        model=model,\n",
    "        nb_actions=n_action,\n",
    "        policy=policy,\n",
    "        memory=memory,\n",
    "        nb_steps_warmup=1000,\n",
    "        gamma=0.95,\n",
    "        target_model_update=100,\n",
    "        delta_clip=0.01,\n",
    "        enable_double_dqn=True,\n",
    "    )\n",
    "    dqn.compile(Adam(learning_rate=0.001), metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "    return dqn\n",
    "\n",
    "def train_model(*, dqn: DQNAgent, train_env, num_steps=10000):\n",
    "    print(\"train_model()...\", num_steps)\n",
    "    dqn.fit(train_env, nb_steps=num_steps)\n",
    "    train_env.close()\n",
    "\n",
    "def eval_model(*, agent: DQNAgent, eval_env, opponent: Player, num_eval_episodes=100):\n",
    "    print(\"eval_model()...\")\n",
    "\n",
    "    eval_env.reset_env(restart=True, opponent=opponent)\n",
    "    print(f\"Results against {opponent.username}:\")\n",
    "    agent.test(eval_env, nb_episodes=num_eval_episodes, verbose=False, visualize=False)\n",
    "    print(\n",
    "        f\"DQN Evaluation: {eval_env.n_won_battles} victories out of {eval_env.n_finished_battles} episodes\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_rl_env_random()...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardzhan/opt/miniconda3/envs/poke/lib/python3.10/site-packages/gym/utils/env_checker.py:186: UserWarning: \u001b[33mWARN: Official support for the `seed` function is dropped. Standard practice is to reset gym environments using `env.reset(seed=<desired seed>)`\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/richardzhan/opt/miniconda3/envs/poke/lib/python3.10/site-packages/gym/utils/env_checker.py:169: UserWarning: \u001b[33mWARN: `return_info` is deprecated as an optional argument to `reset`. `reset`should now always return `obs, info` where `obs` is an observation, and `info` is a dictionarycontaining additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/richardzhan/opt/miniconda3/envs/poke/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_model()...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 21:45:11.806229: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2023-11-29 21:45:11.817913: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_3_1/bias/Assign' id:232 op device:{requested: '', assigned: ''} def:{{{node dense_3_1/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_3_1/bias, dense_3_1/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_rl_env_random()...\n",
      "train_model()... 100000\n",
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    7/10000 [..............................] - ETA: 3:46 - reward: -14.2857  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardzhan/opt/miniconda3/envs/poke/lib/python3.10/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-11-29 21:45:14.208841: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_3/BiasAdd' id:119 op device:{requested: '', assigned: ''} def:{{{node dense_3/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_3/MatMul, dense_3/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-11-29 21:45:14.222919: W tensorflow/c/c_api.cc:305] Operation '{name:'count_3/Assign' id:373 op device:{requested: '', assigned: ''} def:{{{node count_3/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count_3, count_3/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1001/10000 [==>...........................] - ETA: 2:59 - reward: -4.9950"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 21:45:34.224893: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_3_1/BiasAdd' id:237 op device:{requested: '', assigned: ''} def:{{{node dense_3_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_3_1/MatMul, dense_3_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-11-29 21:45:34.294230: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_3/AddN' id:469 op device:{requested: '', assigned: ''} def:{{{node loss_3/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_3/mul, loss_3/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-11-29 21:45:34.311927: W tensorflow/c/c_api.cc:305] Operation '{name:'training/Adam/dense_3/bias/v/Assign' id:743 op device:{requested: '', assigned: ''} def:{{{node training/Adam/dense_3/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/dense_3/bias/v, training/Adam/dense_3/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 240s 24ms/step - reward: -3.9600\n",
      "886 episodes - episode_reward: -44.695 [-100.000, 100.000] - loss: 0.096 - mae: 3.003 - mean_q: -0.473 - mean_eps: 0.456\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 249s 25ms/step - reward: -4.1900\n",
      "929 episodes - episode_reward: -45.102 [-100.000, 100.000] - loss: 0.090 - mae: 1.376 - mean_q: -0.110 - mean_eps: 0.010\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 249s 25ms/step - reward: -4.1700\n",
      "891 episodes - episode_reward: -46.801 [-100.000, 100.000] - loss: 0.093 - mae: 1.324 - mean_q: -0.319 - mean_eps: 0.010\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 254s 25ms/step - reward: -3.5300\n",
      "877 episodes - episode_reward: -40.251 [-100.000, 100.000] - loss: 0.090 - mae: 1.432 - mean_q: -0.550 - mean_eps: 0.010\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 258s 26ms/step - reward: -3.7300\n",
      "893 episodes - episode_reward: -41.769 [-100.000, 100.000] - loss: 0.091 - mae: 1.972 - mean_q: -0.183 - mean_eps: 0.010\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 260s 26ms/step - reward: -3.7900\n",
      "883 episodes - episode_reward: -42.922 [-100.000, 100.000] - loss: 0.092 - mae: 2.172 - mean_q: -0.449 - mean_eps: 0.010\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "  765/10000 [=>............................] - ETA: 3:58 - reward: -6.1438"
     ]
    }
   ],
   "source": [
    "\n",
    "train_env, eval_env = create_rl_env_random()\n",
    "\n",
    "# create the model\n",
    "dqn = create_model(\n",
    "    train_env=train_env,\n",
    "    anneal_steps=10000,\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "# for epoch in range(10):\n",
    "#     print(f\"epoch {epoch}...\")\n",
    "\n",
    "train_env, _ = create_rl_env_random()\n",
    "train_model(\n",
    "    dqn=dqn, \n",
    "    train_env=train_env,\n",
    "    num_steps=100000\n",
    ")\n",
    "\n",
    "# Evaluating the model\n",
    "## recreate eval_env (sometimes breaks if the training takes too long)\n",
    "_, eval_env = create_rl_env_random()\n",
    "eval_model(\n",
    "    agent=dqn,\n",
    "    eval_env=eval_env,\n",
    "    opponent=create_random_player(),\n",
    "    num_eval_episodes=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_rl_env_random()...\n",
      "Results against RandomPlayer 10:\n",
      "DQN Evaluation: 78 victories out of 100 episodes\n"
     ]
    }
   ],
   "source": [
    "opponent = create_random_player()\n",
    "\n",
    "train_env, eval_env = create_rl_env_random()\n",
    "eval_env.reset_env(restart=True, opponent=opponent)\n",
    "print(f\"Results against {opponent.username}:\")\n",
    "dqn.test(eval_env, nb_episodes=100, verbose=False, visualize=False)\n",
    "print(\n",
    "    f\"DQN Evaluation: {eval_env.n_won_battles} victories out of {eval_env.n_finished_battles} episodes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/richardzhan/cs/15888/poke/src/train.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/richardzhan/cs/15888/poke/src/train.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m eval_env\u001b[39m.\u001b[39;49mclose()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/poke/lib/python3.10/site-packages/poke_env/player/openai_api.py:439\u001b[0m, in \u001b[0;36mOpenAIGymEnv.close\u001b[0;34m(self, purge)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_battle \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mcurrent_battle\n\u001b[1;32m    436\u001b[0m closing_task \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mrun_coroutine_threadsafe(\n\u001b[1;32m    437\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop_challenge_loop(purge\u001b[39m=\u001b[39mpurge), POKE_LOOP\n\u001b[1;32m    438\u001b[0m )\n\u001b[0;32m--> 439\u001b[0m closing_task\u001b[39m.\u001b[39;49mresult()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/poke/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/poke/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poke",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
