{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import numpy as np\n",
    "from gym.spaces import Box, Space\n",
    "from gym.utils.env_checker import check_env\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from tabulate import tabulate\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers.legacy import Adam\n",
    "\n",
    "from poke_env.environment.abstract_battle import AbstractBattle\n",
    "from poke_env.environment import Move\n",
    "from poke_env.player import (\n",
    "    Player,\n",
    "    Gen8EnvSinglePlayer,\n",
    "    MaxBasePowerPlayer,\n",
    "    ObsType,\n",
    "    RandomPlayer,\n",
    "    SimpleHeuristicsPlayer,\n",
    "    background_cross_evaluate,\n",
    "    background_evaluate_player,\n",
    "    wrap_for_old_gym_api,\n",
    ")\n",
    "from poke_env.data import GenData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/richardzhan/cs/15888/poke\n",
      "\u001b[1m\u001b[36mdata\u001b[m\u001b[m                \u001b[1m\u001b[36mpokemon-showdown\u001b[m\u001b[m    \u001b[1m\u001b[36mscripts\u001b[m\u001b[m\n",
      "poke.code-workspace \u001b[1m\u001b[36mpython\u001b[m\u001b[m              \u001b[1m\u001b[36msrc\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "%cd ~/cs/15888/poke\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from rzlib.env import embed\n",
    "from rzlib.env.simple_rl_player import SimpleRLPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_battle_format = \"gen8ou\"\n",
    "_team1_fname = \"./data/team1.txt\"\n",
    "_team2_fname = \"./data/team2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_team(fname):\n",
    "    # load_bot()\n",
    "    with open(fname, \"r\") as f:\n",
    "        return \"\".join(f.readlines())\n",
    "\n",
    "def create_random_player():\n",
    "    return RandomPlayer(battle_format=_battle_format, team=load_team(_team1_fname))\n",
    "\n",
    "def create_max_power_player():\n",
    "    return MaxBasePowerPlayer(battle_format=_battle_format, team=load_team(_team1_fname))\n",
    "\n",
    "def create_rl_player(opponent, **kwargs):\n",
    "    return SimpleRLPlayer(\n",
    "        battle_format=_battle_format,\n",
    "        opponent=opponent,\n",
    "        start_challenging=True,\n",
    "        team=load_team(_team2_fname),\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "def create_rl_env_random(sanity_check=True):\n",
    "    print(\"create_rl_env_random()...\")\n",
    "    if sanity_check:\n",
    "        # First test the environment to ensure the class is consistent\n",
    "        # with the OpenAI API\n",
    "        sanity_check_env = create_rl_player(opponent=create_random_player())\n",
    "        check_env(sanity_check_env)\n",
    "        sanity_check_env.close()\n",
    "\n",
    "    # Create one environment for training and one for evaluation\n",
    "    train_env = create_rl_player(opponent=create_random_player())\n",
    "    train_env = wrap_for_old_gym_api(train_env)\n",
    "\n",
    "    eval_env = create_rl_player(opponent=create_random_player())\n",
    "    eval_env = wrap_for_old_gym_api(eval_env)\n",
    "\n",
    "    return train_env, eval_env\n",
    "\n",
    "\n",
    "def create_model(train_env, nb_steps=10000):\n",
    "    print(\"create_model()...\")\n",
    "    # Compute dimensions\n",
    "    n_action = train_env.action_space.n\n",
    "    input_shape = (1,) + train_env.observation_space.shape\n",
    "\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation=\"elu\", input_shape=input_shape))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation=\"elu\"))\n",
    "    model.add(Dense(n_action, activation=\"linear\"))\n",
    "\n",
    "    # Defining the DQN\n",
    "    memory = SequentialMemory(limit=nb_steps, window_length=1)\n",
    "\n",
    "    policy = LinearAnnealedPolicy(\n",
    "        EpsGreedyQPolicy(),\n",
    "        attr=\"eps\",\n",
    "        value_max=1.0,\n",
    "        value_min=0.05,\n",
    "        value_test=0.0,\n",
    "        nb_steps=nb_steps,\n",
    "    )\n",
    "\n",
    "    dqn = DQNAgent(\n",
    "        model=model,\n",
    "        nb_actions=n_action,\n",
    "        policy=policy,\n",
    "        memory=memory,\n",
    "        nb_steps_warmup=1000,\n",
    "        gamma=0.99,\n",
    "        target_model_update=1000,\n",
    "        delta_clip=0.01,\n",
    "        enable_double_dqn=True,\n",
    "    )\n",
    "    dqn.compile(Adam(learning_rate=0.00025), metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "    return dqn\n",
    "\n",
    "def train_model(*, dqn: DQNAgent, train_env):\n",
    "    print(\"train_model()...\", dqn.policy.nb_steps)\n",
    "    dqn.fit(train_env, nb_steps=dqn.policy.nb_steps)\n",
    "    train_env.close()\n",
    "\n",
    "def eval_model(*, agent: DQNAgent, eval_env, opponent: Player, num_eval_episodes=100):\n",
    "    print(\"eval_model()...\")\n",
    "\n",
    "    eval_env.reset_env(restart=True, opponent=opponent)\n",
    "    print(f\"Results against {opponent.username}:\")\n",
    "    agent.test(eval_env, nb_episodes=num_eval_episodes, verbose=False, visualize=False)\n",
    "    print(\n",
    "        f\"DQN Evaluation: {eval_env.n_won_battles} victories out of {eval_env.n_finished_battles} episodes\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_rl_env_random()...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardzhan/opt/miniconda3/envs/poke/lib/python3.10/site-packages/gym/utils/env_checker.py:186: UserWarning: \u001b[33mWARN: Official support for the `seed` function is dropped. Standard practice is to reset gym environments using `env.reset(seed=<desired seed>)`\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/richardzhan/opt/miniconda3/envs/poke/lib/python3.10/site-packages/gym/utils/env_checker.py:169: UserWarning: \u001b[33mWARN: `return_info` is deprecated as an optional argument to `reset`. `reset`should now always return `obs, info` where `obs` is an observation, and `info` is a dictionarycontaining additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/richardzhan/opt/miniconda3/envs/poke/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_model()...\n",
      "train_model()... 100000\n",
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    1/10000 [..............................] - ETA: 8:30 - reward: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 10:02:15.793919: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2023-11-29 10:02:15.803550: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_2_1/bias/Assign' id:184 op device:{requested: '', assigned: ''} def:{{{node dense_2_1/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_2_1/bias, dense_2_1/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/Users/richardzhan/opt/miniconda3/envs/poke/lib/python3.10/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-11-29 10:02:15.938470: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_2/BiasAdd' id:95 op device:{requested: '', assigned: ''} def:{{{node dense_2/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_2/MatMul, dense_2/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-11-29 10:02:15.947380: W tensorflow/c/c_api.cc:305] Operation '{name:'count_3/Assign' id:315 op device:{requested: '', assigned: ''} def:{{{node count_3/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count_3, count_3/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1002/10000 [==>...........................] - ETA: 3:01 - reward: -0.9116"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 10:02:35.990326: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_2_1/BiasAdd' id:189 op device:{requested: '', assigned: ''} def:{{{node dense_2_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_2_1/MatMul, dense_2_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-11-29 10:02:36.053861: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_3/AddN' id:411 op device:{requested: '', assigned: ''} def:{{{node loss_3/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_3/mul, loss_3/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-11-29 10:02:36.069298: W tensorflow/c/c_api.cc:305] Operation '{name:'training/Adam/beta_2/Assign' id:575 op device:{requested: '', assigned: ''} def:{{{node training/Adam/beta_2/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/beta_2, training/Adam/beta_2/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6933/10000 [===================>..........] - ETA: 1:07 - reward: -1.0679"
     ]
    }
   ],
   "source": [
    "\n",
    "train_env, eval_env = create_rl_env_random()\n",
    "\n",
    "# create the model\n",
    "dqn = create_model(\n",
    "    train_env=train_env,\n",
    "    nb_steps=100000,\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "train_model(\n",
    "    dqn=dqn, \n",
    "    train_env=train_env,\n",
    ")\n",
    "\n",
    "# Evaluating the model\n",
    "eval_model(\n",
    "    agent=dqn,\n",
    "    eval_env=eval_env,\n",
    "    opponent=create_random_player(),\n",
    "    num_eval_episodes=100,\n",
    ")\n",
    "eval_model(\n",
    "    agent=dqn,\n",
    "    eval_env=eval_env,\n",
    "    opponent=create_max_power_player(),\n",
    "    num_eval_episodes=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/richardzhan/cs/15888/poke/src/train.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/richardzhan/cs/15888/poke/src/train.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m eval_env\u001b[39m.\u001b[39;49mclose()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/poke/lib/python3.10/site-packages/poke_env/player/openai_api.py:439\u001b[0m, in \u001b[0;36mOpenAIGymEnv.close\u001b[0;34m(self, purge)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_battle \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mcurrent_battle\n\u001b[1;32m    436\u001b[0m closing_task \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mrun_coroutine_threadsafe(\n\u001b[1;32m    437\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop_challenge_loop(purge\u001b[39m=\u001b[39mpurge), POKE_LOOP\n\u001b[1;32m    438\u001b[0m )\n\u001b[0;32m--> 439\u001b[0m closing_task\u001b[39m.\u001b[39;49mresult()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/poke/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/poke/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poke",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
